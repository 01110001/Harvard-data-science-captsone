---
output:
  pdf_document:
    toc: false  # Set this to false to suppress the default ToC
    latex_engine: xelatex
    keep_tex: true
header-includes:
  - "\\usepackage{titling}"
  - "\\pretitle{\\begin{center}\\LARGE}"
  - "\\posttitle{\\end{center}\\vfill}"
  - "\\preauthor{\\begin{center}\\large}"
  - "\\postauthor{\\end{center}\\vfill}"
  - "\\predate{\\begin{center}\\large}"
  - "\\postdate{\\end{center}\\vfill\\newpage}"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\title{Harvard Data Science: Predicting Movie Rating with MovieLens dataset}
\author{Maxandre Hebert}
\date{2023-08-08}
\maketitle

\tableofcontents
\newpage


# Introduction

In the rapidly evolving digital landscape, where user-driven content has gained significant prominence, understanding and predicting user behavior is critical for a wide range of applications, from e-commerce to entertainment. The power to accurately predict users' preferences is not just a boon for enhancing user experience but also a strategic advantage for businesses, contributing to their growth and sustainability.

One such area where user preference prediction plays a pivotal role is in the movie industry, especially in online streaming platforms. Personalized movie recommendations can drastically improve user engagement and satisfaction. This study utilizes the MovieLens 10M dataset, a rich trove of user-movie interactions, to predict user ratings, acting as a stepping-stone towards enhanced recommendation systems.

The MovieLens 10M dataset, made available by GroupLens, contains approximately 10 million ratings of 10,000 movies by 72,000 users. These data provide a granular view of user behavior and preferences. By employing modern data analysis and machine learning techniques, we aim to develop a model that accurately predicts a user's rating of a movie, based on their previous ratings and other users' behaviors.

This research paper aims to contribute to the ongoing discussion on user preference prediction and its implications for personalization in digital platforms. It is written to be understandable for a broad audience, whether you are a data science enthusiast, an industry professional, or simply someone interested in how your movie recommendations are shaped.

#Methodology/Analysis In this section we gonna first import the data then clean the data then do the exploratory work.

## Importing the data

```{r eval=FALSE}
##########################################################
# Create edx and final_holdout_test sets 
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

options(timeout = 120)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
# set.seed(1) # if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
validation <- movielens[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
validation_set <- validation %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(validation, validation_set)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, validation, movielens, removed)


```

```{r}
edx <- readRDS("edx.rds")
validation_set <- readRDS("validation_set.rds")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
```

## Data Cleaning

First let's clean the data and check if any missing value are there.

```{r}
  #check if there are any missing data in the trainning set
  print(sum(is.na(edx)))

  #check if there is any missing data in the validation set
  print(sum(is.na(validation_set)))

```

Alright our data set is good we can move on !

## Data Type

```{r}
  str(edx)
```

This trainning dataset contains about 9 million observations with 6 variables each. The variables include 'userId' (integer), 'movieId' (integer), 'rating' (numeric), 'timestamp' (integer), 'title' (character), and 'genres' (character). It appears to be a movie rating dataset where each observation represents a unique rating given by a user to a movie, along with the timestamp of the rating, the title of the movie, and its genres. The 'userId' and 'movieId' suggest that multiple ratings from the same user or for the same movie can be included.

## Data Exploration

### Descriptive Statistics

#### Number of unique movies\|users\|genres

```{r echo=FALSE}
# Print summary of the "edx" dataset
# Create a new data frame to store summary information
df_summary <- data.frame(
  "Number of Movies" = n_distinct(edx$movieId),      # Calculate the number of distinct movies
  "Number of Users" = n_distinct(edx$userId),        # Calculate the number of distinct users
  "Number of movies genres" = n_distinct(edx$genres) # Calculate the number of distinct movie genres
)

# Render the "df_summary" data frame as a nicely formatted table
knitr::kable(df_summary)
```

From the given information, we can see that the original dataset is quite extensive, with ratings for over 10,000 different movies provided by nearly 70,000 unique users. Furthermore, the variety of content is noteworthy, with 797 distinct genres represented. The broad user base and diverse range of movie genres imply a comprehensive coverage of user preferences and movie types, making it valuable for movie recommendation systems, audience segmentation, or studying viewer behavior and trends.

```{r echo=FALSE}
# Top 10 most rated movies: Calculate the top 10 movies with the highest number of ratings.
numRatings <- edx %>% group_by(movieId) %>% 
  summarize(numRatings = n(), movieTitle = first(title)) %>%
  arrange(desc(numRatings)) %>%
  top_n(10, numRatings) %>%
  knitr::kable(caption = "Top 10 most rated movies")

# Top 10 best movies by their ratings: Find the top 10 movies with the highest average ratings.
best.rated <- edx %>% group_by(movieId) %>% 
  summarize(mean_rating = mean(rating), movieTitle = first(title)) %>%
  arrange(desc(mean_rating)) %>%
  top_n(10, mean_rating) %>%
  knitr::kable(caption = "Top 10 best movies by their ratings")

# Top 10 worst movies: Identify the top 10 movies with the lowest average ratings.
worst.rated <- edx %>% group_by(movieId) %>% 
  summarize(mean_rating = mean(rating), movieTitle = first(title)) %>%
  arrange(mean_rating) %>%
  top_n(10, mean_rating) %>%
  knitr::kable(caption = "Top 10 worst movies")



```

### Correlation Analysis

```{r echo=FALSE}
# Calculate the Spearman correlation matrix for "rating," "timestamp," and "userId" columns in the "edx" dataset.
correlation_matrix <- cor(edx[, c("rating", "timestamp", "userId")], method = "spearman")


# Display the correlation matrix in a formatted table using `knitr::kable()`.
knitr::kable(correlation_matrix, caption = "Correlation matrix")


```

Not much to see there, let's go more visual.

### Data Visualization

#### Distribution Plots

```{r echo=FALSE, fig.align="center"}
# Create summarized data frame
# Create a summarized data frame "movie_counts" with the count of each movieId in the "edx" dataset.
movie_counts <- edx %>% count(movieId)

# Create a plot using ggplot to visualize the distribution of the number of ratings per movie.
ggplot(movie_counts, aes(n)) +
  geom_histogram(bins=30, color="black") +
  scale_x_log10() +
  labs(x = "Movies", y = "Number of Ratings", caption = "Source: edx dataset") +
  ggtitle("Distribution of Number of Ratings per Movie (Log Scale)")



```

The distribution plot above illustrates the number of ratings per movie in the "edx" dataset, with the x-axis representing the number of ratings and the y-axis indicating the count of movies falling into each rating count range. The use of a logarithmic scale on the x-axis helps to better visualize the data, especially when there is a wide range of rating counts.

The plot exhibits a right-skewed distribution, also known as a positively skewed distribution. This means that there are a few movies with an exceptionally high number of ratings, while the majority of movies have relatively fewer ratings. In this case, the right tail of the distribution is longer, indicating the presence of a few popular movies that received significantly more ratings compared to the rest of the movies.

The right-skewed nature of the distribution suggests that some movies are much more popular or well-known among users, leading to a higher number of ratings for those movies. On the other hand, less popular or niche movies tend to receive fewer ratings. Understanding the distribution of ratings is essential for developing recommendation systems, as it helps identify highly-rated movies that might be appealing to a broader audience, as well as niche movies that could be of interest to specific user segments.

```{r echo=FALSE}
# Create a new data frame "movie_rating_avg" that calculates the average rating for each movieId in the "edx" dataset.
movie_rating_avg <- edx %>% select(rating, movieId) %>% group_by(movieId) %>% summarise(avg_rating = mean(rating, na.rm = TRUE))

# Create a plot using ggplot to visualize the distribution of average ratings per movie.
ggplot(movie_rating_avg, aes(avg_rating)) +
  geom_histogram(bins = 30, color = "black") +
  labs(x = "Average rating", y = "Number of movies", caption = "Source: edx dataset") +
  ggtitle("Distribution of Average Ratings per Movie")




```

The distribution plot displayed above represents the average ratings for each movie in the "edx" dataset. The x-axis shows the average rating values, while the y-axis indicates the count of movies falling into each average rating range. In this case, the plot demonstrates a left-skewed distribution, also known as a negatively skewed distribution.

A left-skewed distribution suggests that the majority of movies have higher average ratings, with only a few movies receiving lower average ratings. The left tail of the distribution is elongated, indicating the presence of a small number of movies that received significantly lower average ratings compared to the rest. Such a distribution implies that most movies are generally well-received by users, with only a few exceptions being less favored.

```{r echo=FALSE}
# Number of ratings by year: Calculate the count of ratings per year in the "edx" dataset.
n_movie_year <- edx %>%
  select(rating, timestamp) %>%
  mutate(year = year(as.POSIXct(timestamp, origin = "1970-01-01"))) %>%
  group_by(year) %>%
  summarise(count = n())

# Create a plot using ggplot to visualize the number of movie ratings per year.
ggplot(n_movie_year, aes(x = year, y = count)) +
  geom_col() +
  labs(x = "Year", y = "Number of Ratings", 
       title = "Number of Movie Ratings per Year", 
       caption = "Source: edx dataset")



```

This is a plot showing number of rating per year. Not much to see there.

```{r echo=FALSE}
# Average rating by year: Calculate the average rating per year in the "edx" dataset.
mean_movie_year <- edx %>%
  select(rating, timestamp) %>%
  mutate(year = year(as.POSIXct(timestamp, origin = "1970-01-01"))) %>%
  group_by(year) %>%
  summarise(avg_rating = mean(rating))

# Create a plot using ggplot to visualize the average movie ratings per year.
ggplot(mean_movie_year, aes(x = year, y = avg_rating)) +
  geom_col() +
  labs(x = "Year", y = "Average of Ratings", 
       title = "Average of Movie Ratings per Year", 
       caption = "Source: edx dataset")


```

This plot show the average rating per year, it is pretty stable !

```{r echo=FALSE}
# Number of movie ratings by released year: Calculate the count of ratings for each released year in the "edx" dataset.
n_movie_rel_year <- edx %>%
  mutate(year = str_extract(title, "\\((\\d{4})\\)")) %>%
  mutate(year = str_sub(year, 2, -2)) %>%
  group_by(year) %>%
  summarise(n = n())

# Create a plot using ggplot to visualize the number of movie ratings by release year.
ggplot(n_movie_rel_year, aes(x = year, y = n)) +
  geom_col() +
  scale_y_continuous(labels = scales::comma) +  # format y-axis labels with commas for thousands
  scale_x_discrete(breaks = seq(min(n_movie_rel_year$year, na.rm = TRUE), max(n_movie_rel_year$year, na.rm = TRUE), by = 10)) +  # adjust x-axis breaks
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # rotate x-axis labels
  labs(x = "Release Year", y = "Number of Ratings", 
       title = "Number of Movie Ratings by Release Year", 
       caption = "Source: edx dataset")



```

This plot show the number of ratings received movie release year. We can see that the 90s are the most popular movie.

```{r echo=FALSE}
# Average rating by release year: Calculate the average rating for each released year in the "edx" dataset.
mean_movie_rel_year <- edx %>%
  mutate(year = str_extract(title, "\\((\\d{4})\\)")) %>%
  mutate(year = str_sub(year, 2, -2)) %>%
  group_by(year) %>%
  summarise(avg_rating = mean(rating))

# Create a plot using ggplot to visualize the average movie ratings by release year.
ggplot(mean_movie_rel_year, aes(x = year, y = avg_rating)) +
  geom_col() +
  scale_y_continuous(labels = scales::comma) +  # format y-axis labels with commas for thousands
  scale_x_discrete(breaks = seq(min(mean_movie_rel_year$year, na.rm = TRUE), max(mean_movie_rel_year$year, na.rm = TRUE), by = 10)) +  # adjust x-axis breaks
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # rotate x-axis labels
  labs(x = "Release Year", y = "Average of Ratings", 
       title = "Average Movie Ratings by Release Year", 
       caption = "Source: edx dataset")



  



```

In this plot, we see that when the release year is below 1975 the average ratings is higher, but we also know that those movies below 1975 were less rated so their average ratings could be less reliable.

```{r echo=FALSE}
 # Most rated genre by number of ratings (n()):
best_genre <- edx %>%
  select(rating, genres) %>%
  group_by(genres) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(10)

# Create a plot using ggplot to visualize the top 10 genres with the highest number of ratings.
ggplot(best_genre, aes(x = reorder(genres, count), y = count)) +
  geom_col() +
  coord_flip() +
  scale_y_continuous(labels = scales::comma) +
  labs(x = "Genre", y = "Number of Ratings", 
       title = "Top 10 Genres by Number of Ratings", 
       caption = "Source: edx dataset")

```

Finally, in this plot here are the most rated genre.

```{r echo=FALSE}
# Top 10 genre by their average rating:
mean_genre <- edx %>%
  select(rating, genres) %>%
  group_by(genres) %>%
  summarise(mean = mean(rating)) %>%
  arrange(desc(mean)) %>%
  head(10)

# Create a plot using ggplot to visualize the top 10 genres with the highest average ratings.
ggplot(mean_genre, aes(x = reorder(genres, mean), y = mean)) +
  geom_col() +
  coord_flip() +
  scale_y_continuous(labels = scales::comma) +
  labs(x = "Genre", y = "Mean of Ratings", 
       title = "Top 10 Genres by Mean of Ratings", 
       caption = "Source: edx dataset")


```

# Result

## Modeling Approach

### Choice of Model

For this project, RMSE model have been choose for its simplicity.

RMSE is a commonly used metric to measure the performance of regression models, including movie rating prediction models. It calculates the square root of the average of the squared differences between the predicted values and the actual values.

The formula for RMSE is as follows:

$RMSE = \sqrt{\frac{1}{n} \sum (predicted - actual)^2}$

Where:

-   n is the number of data points (movie ratings) in your dataset.

-   Σ represents the sum of the squared differences between the predicted and actual values for each data point.

A lower RMSE value indicates better model performance.

**For this project we aim an RMSE below 0.8649**

### The formula

```{r}
#This is the formula for the RMSE
rmse <- function(true_ratings, predicted_ratings) {
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

```

### Model 1: Uniform Prediction and Least Squares Estimation for Movie Ratings

In the simplest possible recommendation system, we make a uniform prediction of the same rating for all movies, regardless of the user. This approach assumes that any differences in ratings are solely due to random variations. Mathematically, we express our prediction for a user's rating on a movie as follows:

Predicted Rating (Yu,i) = Average Rating (µ) + Random Variation (ϵu,i)

To minimize the Root Mean Square Error (RMSE) and find the best estimate for the user's ratings, we employ the least squares method, which involves averaging all the ratings in the dataset.

```{r}
#First model the mean of all ratings
mean_rating <- mean(edx$rating)
mean_rating

# Now if we test it on the validation test
only_mean_rmse <- rmse(validation_set$rating,mean_rating)
only_mean_rmse

rmse_table_results <- data.frame(Model="Model 1",RMSE=only_mean_rmse)
rmse_table_results %>% knitr::kable(caption="Table of all model result")

```

We can see that the rmse from the last piece of code, using a naive approach didn't work quite well. Actually we are off the track by 1.06 which is not what we want. We want the rmse below 0.8649.

Let's move to the second model.

### Model 2: Incorporating Movie Effects for Improved Movie Rating Predictions

Model 2 introduces the concept of "Movie Effect," represented by the term b_i. The rationale behind this approach is that certain movies tend to receive higher ratings in general, irrespective of individual user preferences. To account for this, we incorporate an additional term, b_i, in the movie rating prediction equation.

The updated prediction equation now becomes Yu,i = µ + b_i + ϵu,i, where:

Yu,i is the predicted rating by user u for movie i. µ is the overall average rating across all movies and users. b_i is the bias term representing the average ranking for movie i. ϵu,i is the individual rating deviation for user u and movie i. By incorporating the movie bias (b_i) into the model, we can capture the inherent differences in movie ratings and better tailor the recommendations. Movies that tend to be rated higher will have a positive b_i value, while movies with lower average ratings will have a negative b_i value. This allows the model to adjust predictions based on the general popularity or appeal of each movie.

```{r}
b_i <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mean_rating))

model_2_pred_ratings <- validation_set %>%
  left_join(b_i, by='movieId') %>%
  mutate(pred = mean_rating + b_i) %>%
  pull(pred)

RMSE_model_2 <- rmse(model_2_pred_ratings, validation_set$rating)
RMSE_model_2

rmse_table_results <- bind_rows(rmse_table_results,
                                data.frame(Model="Model 2",RMSE=RMSE_model_2))

rmse_table_results %>% knitr::kable()

```

We can see our RMSE is improving but still note quite what we want.

Let's move to the next model.

### Model 3: Incorporating User Effects and Enhanced Movie Rating Predictions

In Model 3, we introduce the concept of "User Effect," represented by the term b_u, along with the existing "Movie Effect" (b_i) from Model 2. The motivation behind this approach is that users exhibit substantial variability in their movie preferences and ratings due to individual tastes and preferences.

The updated prediction equation is now $Yu,i = µ + b_i + b_u + ϵu,i,$

where:

-   Yu,i is the predicted rating by user u for movie i.

-   µ is the overall average rating across all movies and users.

-   b_i is the movie bias term representing the average ranking for movie i.

-   b_u is the user bias term representing the individual rating behavior of user u.

-   ϵu,i is the individual rating deviation for user u and movie i.

By incorporating both the user and movie biases into the model, we can account for the personalized preferences of users and the inherent differences in movie ratings. Users with generally higher ratings will have a positive b_u value, while users with lower average ratings will have a negative b_u value. This allows the model to adapt predictions based on both individual user preferences and the popularity of each movie, leading to more accurate and tailored movie recommendations.

```{r}
b_u <- edx %>% 
  left_join(b_i, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mean_rating - b_i))

model_3_pred_ratings <- validation_set %>% 
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  mutate(pred = mean_rating + b_i + b_u) %>%
  pull(pred)

RMSE_model_3 <- rmse(model_3_pred_ratings, validation_set$rating)
RMSE_model_3

rmse_table_results <- bind_rows(rmse_table_results,
                                data.frame(Model="Model 3",RMSE=RMSE_model_3))

rmse_table_results %>% knitr::kable()
```

We are improving and really close to our objective.

### Model 4: Regularized Movie and User Effects for Improved Recommendation Predictions

Let's start first to understand what is regularization.

Imagine you are trying to hit a target with a bow and arrow. You're practicing, and you're getting really good at hitting the target exactly where you aim. This is like creating a model that fits your training data perfectly.

But then the wind starts blowing. You shoot the arrow exactly the way you've been practicing, but the wind carries it off-course, and you miss. This is like applying your model to new data and finding out it doesn't perform well. This is an example of overfitting - your model was too perfectly fitted to the specific conditions of your training data and failed when those conditions changed.

Regularization is like deciding not to aim exactly at the target, but a little off to one side, taking into account that there might be some wind that will push your arrow towards the target. You're making your shot a little "worse" on purpose, because it makes you better able to handle unexpected conditions.

In the world of machine learning, "making your shot a little worse" means preventing your model from getting too complex. A complex model can fit the training data perfectly, but fail on new data, just like the perfect shot fails when the wind starts blowing. Regularization does this by adding a penalty to the model for complexity.

Here what the code do :

lambdas is a vector of lambda values, which controls the level of regularization in the model. Higher lambda values imply a simpler model (more regularization) and lower values imply a more complex model (less regularization).

For each lambda value, the code calculates two bias terms: b_i (movie effect) and b_u (user effect). These terms are calculated using a method that takes into account regularization. The lambda value in the denominator serves as the regularization parameter, controlling the influence of the number of ratings each movie or user has.

The b_i term is calculated by grouping the training data by movieId, and for each movie, calculating the average of the deviations of the movie's ratings from the overall average rating (mu), adjusted by the regularization term.

The b_u term is calculated in a similar way, but it's grouped by userId and also takes into account the already-calculated b_i term. This means it's the average deviation of the user's ratings from the average ratings of the movies they rated, again adjusted by the regularization term.

Then, the code calculates the predicted ratings for the test data by adding up mu, b_i, and b_u for each rating. It's essentially creating a very simple model where the predicted rating is the overall average rating plus a movie effect and a user effect.

The last step is to evaluate the model by calculating the RMSE between the predicted ratings and the actual ratings in the test data.

Here is the code :

```{r}

lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(lambda){
  
  b_i <- edx %>% # Movie Effect (Regularized)
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mean_rating)/(n()+lambda))
  
  b_u <- edx %>% # Movie and User Effect (Regularized)
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mean_rating)/(n()+lambda))
  
  model_4_pred_ratings <- validation_set %>% # Predicted Ratings based on Model 4
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mean_rating + b_i + b_u) %>%
    pull(pred) # We want to use predictions based on Model 4
  
  return(rmse(model_4_pred_ratings, validation_set$rating)) # We want RMSE
})


lambda_min <- lambdas[which.min(rmses)]
lambda_min
qplot(lambdas,rmses)
```

```{r}
RMSE_model_4 <- min(rmses)
RMSE_model_4

rmse_table_results <- bind_rows(rmse_table_results,
                                data.frame(Model="Model 4",RMSE=RMSE_model_4))


```

BAM we got what we want !

### Model Performance

#### Model RMSE score

```{r echo=FALSE}
rmse_table_results %>% knitr::kable()



```

We can see the best model was model 4.

# Conclusion

In this movie recommendation algorithm project, an iterative approach was taken to achieve a low Root Mean Square Error (RMSE). Starting with a basic approach, different models were developed, incorporating MovieLens features like movie and user effects. Regularization techniques were also applied to improve the RMSE value.

Despite the progress made, there are still opportunities for improvement. By considering additional predictors such as "genres" and "releaseYear" and using a larger dataset like MovieLens 25M, further RMSE reduction is achievable.

Though the Regularized Movie + User Effects Model has been successful in achieving the primary goal of the MovieLens Project, certain limitations need addressing. The hardware constraints and processing power have restricted exploring more advanced machine learning models like k-NN. Moreover, a deeper understanding of the dataset could have allowed for better fine-tuning of the models.

Looking ahead, overcoming these limitations with the support of peer reviewers and more powerful hardware would lead to more refined movie recommendation algorithms, taking us closer to the project's full potential.
